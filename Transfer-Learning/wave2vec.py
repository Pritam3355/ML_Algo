# -*- coding: utf-8 -*-
"""wave2vec.ipynb
Automatically generated by Colaboratory.
"""

#!pip install pytube pydub

from pytube import YouTube
from pydub import AudioSegment
import datetime

def download_audio(url=None):
    start=datetime.datetime.now()
    if url!=None:
      yt=YouTube(url)
      print(yt.title)
      audios=list(yt.streams.filter(only_audio=True))
      for idx,audio in enumerate(audios):
        print(f"{idx} ------> {audio}")
      strm=int(input("Enter which stream you want to dowanload :"))
      src_audio=audios[strm].download()
      print(src_audio)
      
    else:
      print("Please provide a video url")
    print("Time taken to download(in sec) : ",datetime.datetime.now()-start)

url="https://www.youtube.com/watch?v=MihlCysVWNs"
download_audio(url)

import os
from scipy.io import wavfile

def convert_audio(): 
  
  format_list=["mp4","mp3","webm"]
  src="-1"
  for file in os.listdir():
      extensions=file.split(".")
      if extensions[-1] in format_list:
        src=extensions[-1]
        name=extensions[-2]
  if src=="-1":
    src=input("Type Extension of your source audio")
  dst="wav"
  sound = AudioSegment.from_file(name+"."+src,format=src)
  sound = sound.set_frame_rate(16000)
  sound.export(name+"."+dst,format=dst)
  sr,_= wavfile.read(name+"."+dst)
  print(f"Currect sample rate is {sr}Hz & required sample rate is 16000HZ")
  target=""
  for file in os.listdir():
    if file.endswith("wav"):
      target="".join(os.getcwd()+"/"+file)
  if len(target)==0:
    target=input("Please provide path for source audio(.wav)")
  return target

target = convert_audio()
print(target)

!pip install huggingsound

import torch
from huggingsound import SpeechRecognitionModel

model=SpeechRecognitionModel("jonatasgrosman/wav2vec2-large-xlsr-53-english")

import librosa
import soundfile as sf

def make_chunk_transcribe():
    ## break the complete audio in block of 30 for faster processing
    stream = librosa.stream(target,block_length=30,frame_length=16000,hop_length=16000)
    audio_path=[] ## store chunks of audio file path 
    for idx,speech in enumerate(stream):
      sf.write(f'chunk_{idx}.wav', speech, 16000)
      audio_path.append(f'{os.getcwd()}/chunk_{idx}.wav') 
    #print(audio_path)
    transcriptions = model.transcribe(audio_path) ## run transcription for each audio chunk
    converted_text=""
    for item in transcriptions:
      converted_text += ''.join(item['transcription'])

    return converted_text

converted_text = make_chunk_transcribe()
#converted_text

from transformers import pipeline

## for smaller test we don't need chunking at all , this may downgrade the performance

def chunk_summarization(word_length=500):
    summarization=pipeline('summarization')
    chunk_size=int(len(converted_text)/word_length)
    summarized_text=[] ## 
    for i in range(0,chunk_size+1):
      start=i*word_length
      end=(i+1)*word_length
      out=summarization(converted_text[start:end],min_length=5,max_length=20)
      out=out[0]['summary_text'] ## out=[{}]
      summarized_text.append(out)
    return summarized_text

summarized_text=chunk_summarization(500)
#summarized_text

